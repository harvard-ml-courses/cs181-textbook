\chapter{Mixture Models}
The real world often generates observable data that falls into a combination of unseen categories. For example, at a specific moment in time I could record sound waves on a busy street that come from a combination of cars, pedestrians, and animals. If I were to try to model my data points, it would be helpful if I could group them by source, even though I didn't observe where each sound wave came from individually.

In this chapter we explore what are known as mixture models. Their purpose is to handle data generated by a combination of unobserved categories. We would like to discover the properties of these individual categories and determine how they mix together to produce the data we observe. We consider the statistical ideas underpinning mixture models, as well as how they can be used in practice.

\section{Motivation}
Mixture models are used to model data involving \textit{latent variables}.

\begin{definition}{Latent Variable}{latent-variable}
    A latent variable is a piece of data that is not observed, but that influences the observed data. We often wish to create models that capture the behavior of our latent variables.
\end{definition}

We are sometimes unable to observe all the data present in a given system. For example, if we measure the snout length of different animals but only get to see the snout measurements themselves, the latent variable would be the type of animal we are measuring for each data point. For most data generating processes, we will only have access to a portion of the data and the rest will be hidden from us. However, if we can find some way to also model the latent variables, our model will potentially be much richer, and we will also be able to probe it with more interesting questions. To build some intuition about latent variable models, we present a simple directed graphical model with a latent variable $\textbf{z}_n$ in Figure \ref{fig:lvm-dgm}.

\begin{figure}
    \centering
    \includegraphics[width=0.3\paperwidth]{../MixtureModels/fig/latent-variable-dgm.png}
    \caption{Directed graphical model with a latent variable $\textbf{z}$.}
    \label{fig:lvm-dgm}
\end{figure}

One common means of modeling data involving latent variables, and the topic of this chapter, is known as a \textit{mixture model}.

\begin{definition}{Mixture Model}{mixture-model}
    A mixture model captures the behavior of data coming from a combination of different distributions.
\end{definition}

At a high level, a mixture model operates under the assumption that our data is generated by first sampling a discrete class, and then sampling a data point from within that category according to the distribution for that category. For the example of animal snouts, we would first sample a species of animal, and then based on the distribution of snout lengths in that species, we would sample an observation to get a complete data point.

Probabilistically, sampling a class (which is our latent variable, since we don't actually observe it) happens according to a Categorical distribution, and we typically refer to the latent variable as $\textbf{z}$. Thus:
\begin{align*}
    p(\textbf{z} = C_{k} ; \boldsymbol{\theta}) = \theta_{k}
\end{align*}
where $C_{k}$ is class $k$, and $\boldsymbol{\theta}$ is the parameter to the Categorical distribution that specifies the probability of drawing each class. We write the latent variable in bold $\textbf{z}$ because  we will typically
consider it to be one-hot encoded (of dimension $K$, for $K$ classes).
Then, once we have a class, we have a distribution for the observed data point coming from that class:
\begin{align*}
    p(\textbf{x} | \textbf{z} = C_{k}; \textbf{w})
\end{align*}
\readernote{The distribution given by $p(\textbf{x} | \textbf{z} = C_{k}; \textbf{w})$ is known as the \textit{class-conditional distribution}.}
This distribution depends on the type of data we are observing, and is parameterized by an arbitrary parameter $\textbf{w}$ whose form depends on what is chosen as the class-conditional distribution. For the case of snout lengths, and many other examples, this conditional distribution is often modeled using a Gaussian distribution, in which case our model is known as a \textbf{Gaussian Mixture Model}. We will discuss Gaussian Mixture Models in more detail later in the chapter.

If we can effectively model the distribution of our observed data points and the latent variables responsible for producing the data, we will be able to ask interesting questions of our model. For example, upon observing a new data point $\textbf{x}'$ we will be able to produce a probability that it came from a specific class $\textbf{z}' = C_k$ using Bayes' rule and our model parameters:
\begin{align*}
    p(\textbf{z}' = C_k | \textbf{x}') = \frac{p(\textbf{x}' | \textbf{z}' = C_{k}; \textbf{w})p(\textbf{z}' = C_{k} ; \boldsymbol{\theta})}{\sum_{k'} p(\textbf{x}' | \textbf{z}' = C_{k'}; \textbf{w})p(\textbf{z}' = C_{k'} ; \boldsymbol{\theta})}
\end{align*}
Furthermore, after modeling the generative process, we will be able to generate new data points by sampling from our categorical class distribution, and then from the class-conditional distribution for that category:
\begin{align*}
    \textbf{z} \sim Cat(\boldsymbol{\theta}) \\
    \textbf{x} \sim p(\textbf{x} | \textbf{z} = C_{k}; \textbf{w})
\end{align*}
Finally, it will also be possible for us to get a sense of the cardinality of $\textbf{z}$ (meaning the number of classes our data falls into), even if that was not something we were aware of a priori.

\begin{mlcube}{Mixture Models}
The classes of data \textbf{z} in a mixture model will typically be discrete. Notice also that this is an unsupervised technique: while we have a data set $\textbf{X}$ of observations, our goal is not to make predictions. Rather, we are trying to model the generative process of this data by accounting for the latent variables that generated the data points. Finally, this is a probabilistic model both for the latent variables and for our observed data.
\begin{center}
    \begin{tabular}{c|c|c}
    \textit{\textbf{Domain}} & \textit{\textbf{Training}} & \textit{\textbf{Probabilistic}} \\
    \hline
    Discrete & Unsupervised & Yes \\
    \end{tabular}
\end{center}
\end{mlcube}

\section{Applications}
Since much of the data we observe in our world has some sort of unobserved category associated with it, there are a wide variety of applications for mixture models. Here are just a few:
\begin{enumerate}
    \item Handwriting image recognition. The categories are given by the characters (letters, numbers, etc.) and the class-conditional is a distribution over what each of those characters might look like.
    \item Noise classification. The categories are given by the source of a noise (e.g. we could have different animal noises), and the class-conditional is a distribution over what the sound waves for each animal noise look like.
    \item Vehicle prices. The categories are given by the brand of vehicle (we could alternatively categorize by size, safety, year, etc.), and the class-conditional is a distribution over the price of each brand.
\end{enumerate}

\section{Fitting a Model}
We've defined the general form of a mixture model: we have a distribution $p(\textbf{z}; \boldsymbol{\theta})$ over our classes and a distribution $p(\textbf{x}|\textbf{z} = C_k; \textbf{w})$ as our class-conditional distribution. A natural approach would be to compute the maximum likelihood values for our parameters $\boldsymbol{\theta}$ and $\textbf{w}$. Let's consider how we might go about this for a mixture model.

\subsection{Maximum Likelihood for Mixture Models}
Our goal is to maximize the likelihood of our observed data. Because we don't actually observe the latent variables $\textbf{z}_n$ which determine the class of each observed data point $\textbf{x}_n$, we can simply sum over the possible classes for each of our $N$ data points as follows:
\begin{align*}
    p(\textbf{X}; \boldsymbol{\theta}, \textbf{w}) = \prod_{n=1}^{N} \sum_{k=1}^{K} p(\textbf{x}_{n}, z_{n, k}; \boldsymbol{\theta}, \textbf{w})
\end{align*}

This uses $p(\textbf{x}_n; \boldsymbol{\theta}, \textbf{w})=\sum_k p(\textbf{x}_n, z_{n,k}; \boldsymbol{\theta}, \textbf{w})$ (marginalizing out over the latent class).
Taking the logarithm to get our log-likelihood as usual:
\begin{align} \label{intractable-log-likelihood}
    \log p(\textbf{X}; \boldsymbol{\theta}, \textbf{w}) = \sum_{n=1}^{N} \log \bigg[ \sum_{k=1}^{K} p(\textbf{x}_{n}, z_{n, k}; \boldsymbol{\theta}, \textbf{w}) \bigg]
\end{align}
It may not be immediately obvious, but under this setup, the maximum likelihood calculation for our parameters $\boldsymbol{\theta}$ and $\textbf{w}$ is now intractable. The summation over the $K$ classes of our latent variable $\textbf{z}_{n}$, which is required because we don't actually observe those classes, is inside of the logarithm, which prevents us from arriving at
an analytical solution (it may be helpful to try to solve this yourself, you'll realize that consolidating a summation inside of a logarithm is not possible). You could still try to use gradient descent, but the problem is non-convex and we'll see a much more elegant approach. The rest of this chapter will deal with how we can optimize our mixture model in the face of this challenge.

\subsection{Complete-Data Log Likelihood}

We have a problem with computing the MLE for our model parameters. If we only knew which classes our data points came from, i.e., if we had $\textbf{z}_n$ for each example $n$, then
we would be able to calculate $\log p(\textbf{x}, \textbf{z})$ with relative ease because we would no longer require a summation inside the logarithm:
%
\begin{align} \label{complete-data-log-likelihood}
    \log p(\textbf{X}, \textbf{Z}) &= \sum_{n=1}^{N} \log p(\textbf{x}_n, \textbf{z}_n; \boldsymbol{\theta}, \textbf{w}) \\
    &= \sum_{n=1}^{N} \log[p(\textbf{x}_n | \textbf{z}_n; \textbf{w}) p(\textbf{z}_n; \boldsymbol{\theta})] \\
    &= \sum_{n=1}^{N} \log p(\textbf{x}_n | \textbf{z}_n; \textbf{w}) + \log p(\textbf{z}_n; \boldsymbol{\theta}) 
\end{align}

Notice that because we've now observed $\textbf{z}_{n}$, we don't have to marginalize over its possible values. This motivates an interesting approach that takes advantage of our ability to work with $p(\textbf{x}, \textbf{z})$ if we only knew $\textbf{z}$.

The expression p(\textbf{x}, \textbf{z}) is known as the \textit{complete-data likelihood} because it assumes that we have both our observation $\textbf{x}$ and the class $\textbf{z}$ that $\textbf{x}$ came from. Our ability to efficiently calculate the complete-data log likelihood $\log p(\textbf{x}, \textbf{z})$ is the crucial piece of the algorithm we will present to optimize our mixture model parameters. This algorithm is known as \textbf{Expectation-Maximization}, or \textbf{EM} for short.

\section{Expectation-Maximization (EM)}
The motivation for the EM algorithm, as presented in the previous section, is that we do not have a closed form optimization for our mixture model parameters due to the summation inside of the logarithm. This summation was required because we didn't observe a crucial piece of data, the class $\textbf{z}$, and therefore we had to sum over its values.

EM uses an iterative approach to optimize our model parameters. It proposes a soft value for $\textbf{z}$ using an expectation calculation (we can think about this as giving a distribution on $\textbf{z}_n$ for each $n$), and then based on that proposed value, it maximizes the {\em expected} complete-data log likelihood with respect to the model parameters $\boldsymbol{\theta}$ and $\textbf{w}$ via a standard MLE procedure.

Notice that EM is composed of two distinct steps: an ``E step'' that finds the expected value of the latent class variables given the current set of parameters, and an ``M step'' that improves the model parameters by maximizing expected complete-data log likelihood given these soft assignments to class variables. These two steps give the algorithm its name, and more generally, this type of approach is also referred to as \textbf{coordinate ascent}. The idea behind coordinate ascent is that we can replace a hard problem (maximizing the log likelihood for our mixture model directly) with two easier problems, namely the E- and M-step. We alternate between the two easier problems, executing each of them until we reach a point of convergence or decide that we've done enough. We may also restart because EM will provide a local but not global optimum.

We'll walk through the details of each of these two steps and then tie them together with the complete algorithm.

\readernote{K-Means, an algorithm we discussed in the context of clustering, is also a form of coordinate ascent. K-Means is sometimes referred to as a  ``maximization-maximization'' algorithm because we iteratively maximize our assignments (by assigning each data point to just a single cluster) and then update our cluster centers to maximize their likelihood with respect to the new assignments. That is, it does a ``max'' in place of the E-etep, making a hard rather than soft assignment.}

\subsection{Expectation Step}

The purpose of the E-step is to find expected values of the latent variables $\textbf{z}_n$ for each example given the current parameter values. Let's consider what this looks like with a concrete example.

Let's say our data points $\textbf{x}_n$ can come from one of three classes. Then, we can represent the latent variable $\textbf{z}_n$ associated with each data point using a one-hot encoded vector. For example, if $\textbf{z}_n$ came from class $C_1$, we would denote this:
\begin{align*}
    \textbf{z}_n = 
        \begin{bmatrix}
            1 \\
            0 \\
            0 \\
        \end{bmatrix}
\end{align*}
As we've already explained, we don't know the true value of this latent variable. Instead, we will compute its conditional expectation based on the current setting of our model parameters and our observed data $\textbf{x}_n$. We denote the expectation of our latent variables as $\textbf{q}_n$, and we calculate them as follows:
\begin{align*}
    \textbf{q}_n = \mathbb{E}[\textbf{z}_n | \textbf{x}_n] &= \begin{bmatrix}
            p(\textbf{z}_n = C_1 | \textbf{x}_n; \boldsymbol{\theta}, \textbf{w}) \\
            p(\textbf{z}_n = C_2 | \textbf{x}_n; \boldsymbol{\theta}, \textbf{w}) \\
            p(\textbf{z}_n = C_3 | \textbf{x}_n; \boldsymbol{\theta}, \textbf{w}) \\
        \end{bmatrix} 
        \propto \begin{bmatrix}
            p(\textbf{x}_n | \textbf{z}_n = C_1; \textbf{w})p(\textbf{z}_n = C_1; \boldsymbol{\theta}) \\
            p(\textbf{x}_n | \textbf{z}_n = C_2; \textbf{w})p(\textbf{z}_n = C_2; \boldsymbol{\theta}) \\
            p(\textbf{x}_n | \textbf{z}_n = C_3; \textbf{w})p(\textbf{z}_n = C_3; \boldsymbol{\theta}) \\
        \end{bmatrix} \\
\end{align*}

The expectation of a 1-hot encoded vector is equivalent to a distribution on the values that the latent variable might take on.
Notice that we can switch from proportionality in our $\textbf{q}_n$ values to actual probabilities by simply dividing each unnormalized value by the sum of all the unnormalized values. Then, our $\textbf{q}_n$ values will look something like the following, where a larger number indicates a stronger belief that the data point $\textbf{x}_n$ came from that class:
\begin{align*}
    \textbf{q}_n = \begin{bmatrix}
            0.8 \\
            0.1 \\
            0.1 \\
        \end{bmatrix}
\end{align*}
There are two important things to note about the expectation step. First, the model parameters $\boldsymbol{\theta}$ and \textbf{w} are held fixed. We're computing the expectation of our latent variables based on the current setting of those model parameters. Those parameters are randomly initialized if this is our first time running the expectation step.

Second, we have a value of $\textbf{q}_n$ for every data point $\textbf{x}_n$ in our data set. As a result, $\textbf{q}_n$ are sometimes called ``local parameters,'' since there is one assigned to each data point. This is in contrast to our model parameters $\boldsymbol{\theta}$ and \textbf{w}, which are ``global parameters.'' The size of the global model parameters doesn't fluctuate based on the size of our data set.

After performing the E-step, we now have an expectation for our latent variables, given by $\textbf{q}_n$. In the maximization step, which we describe next, we use these $\textbf{q}_n$ values to improve our global parameters.

\subsection{Maximization Step}

After the expectation step, we have a $\textbf{q}_n \in [0,1]^K$ (and summing to one) associated with each data point $\textbf{x}_n$, which describes our belief that the data point came from each class $C_k$. Now that we have these expected `class assignments', it's possible for us to maximize the expected complete-data likelihood with respect to our model parameters $\boldsymbol{\theta}$ and $\textbf{w}$.

Recall that optimizing our parameters using the complete-data log likelihood is tractable because we avoid summing over the classes inside of the logarithm. Although we do not have the actual complete-data, since we don't know the true $\textbf{z}_n$ values, we  now have a distribution over these latent variables (given by $\textbf{q}_n$).

Notice that our $\textbf{q}_n$ values are `soft' assignments - meaning that unlike the $\textbf{z}_n$ values, which are one-hot encodings of assignments to a class, the $\textbf{q}_n$ values have a probability that a data point $\textbf{x}_n$ came from each class.
Recall the expression for complete-data log likelihood:
%
\begin{align}
    \log p(\textbf{X}, \textbf{Z}) = \sum_{n=1}^{N} \log p(\textbf{x}_n | \textbf{z}_n; \textbf{w}) + \log p(\textbf{z}_n; \boldsymbol{\theta}) 
\end{align}


We work instead with the expected complete-data log likelihood, using $\textbf{q}_n$ to provide the distribution on
$\textbf{z}_n$ for each example $n$:
%
\begin{align} \label{tractable-eqn}
    \mathrm{E}_{\textbf{z}_n | \textbf{x}_n} [\log p(\textbf{X}, \textbf{Z})] &= \mathrm{E}_{\textbf{z}_n | \textbf{x}_n} \bigg[ \sum_{n=1}^{N} \log p(\textbf{x}_n | \textbf{z}_n; \textbf{w}) + \log p(\textbf{z}_n; \boldsymbol{\theta}) \bigg] \\
    &= \sum_{n=1}^{N} \mathrm{E}_{\textbf{z}_n | \textbf{x}_n} \bigg[ \log p(\textbf{x}_n | \textbf{z}_n ; \textbf{w}) + \log p(\textbf{z}_n ; \boldsymbol{\theta}) \bigg] \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} p(\textbf{z}_n = C_k | \textbf{x}_n) \big( \log p(\textbf{x}_n | \textbf{z}_n = C_k; w) + \log p(\textbf{z}_n = C_k; \boldsymbol{\theta}) \big) \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} q_{n, k} \big( \log p(\textbf{x}_n | \textbf{z}_n = C_k; \textbf{w}) + \log p(\textbf{z}_n = C_k; \boldsymbol{\theta}) \big) 
\end{align}

Notice the crucial difference between this summation and that of Equation \ref{intractable-log-likelihood}: the summation over the classes is now outside of the logarithm! Recall that using the log-likelihood directly was intractable precisely because the summation over the classes was inside of the logarithm. This maximization became possible by taking the expectation over our latent variables (using the values we computed in the E-step), which moved the summation over the classes outside of the logarithm.

We can now complete the M-step by maximizing Equation \ref{tractable-eqn} with respect to our model parameters $\boldsymbol{\theta}$ and $\textbf{w}$. This has an analytical solution. We take the derivative with respect to the parameter of interest, set to 0, solve, and update the parameter with the result.

\subsection{Full EM Algorithm}

Now that we have a grasp on the purpose of the EM algorithm, as well as an understanding of the expectation and maximization steps individually, we are ready to put everything together to describe the entire EM algorithm.

\begin{itemize}
    \item[1.] Begin by initializing our model parameters $\textbf{w}$ and $\boldsymbol{\theta}$, which we can do at random. Since the EM algorithm is performed over a number of iterative steps, we will denote these initial parameter values $\textbf{w}^{(0)}$ and $\boldsymbol{\theta}^{(0)}$. We will increment those values as the algorithm proceeds.
    \item[2.] E-step: compute the values of $\textbf{q}_n$ based on the current setting of our model parameters.
    \begin{align*}
        \textbf{q}_n = \mathbb{E}[\textbf{z}_n | \textbf{x}_n] = \begin{bmatrix}
                p(\textbf{z}_n = C_1 | \textbf{x}_n; \boldsymbol{\theta}^{(i)}, \textbf{w}^{(i)}) \\
                \vdots \\
                p(\textbf{z}_n = C_K | \textbf{x}_n; \boldsymbol{\theta}^{(i)}, \textbf{w}^{(i)}) \\
            \end{bmatrix} 
            \propto \begin{bmatrix}
            p(\textbf{x}_n | \textbf{z}_n = C_1; \textbf{w}^{(i)})p(\textbf{z}_n = C_1; \boldsymbol{\theta}^{(i)}) \\
            \vdots \\
            p(\textbf{x}_n | \textbf{z}_n = C_K; \textbf{w}^{(i)})p(\textbf{z}_n = C_K; \boldsymbol{\theta}^{(i)}) \\
        \end{bmatrix} \\
    \end{align*}
  \item[3.] M-step: compute the values of $\textbf{w}$ and $\boldsymbol{\theta}$ that maximize our expected complete-data log likelihood for the current setting of the values of $\textbf{q}_n$:
    %
    \begin{align}
        \textbf{w}^{(i + 1)}, \boldsymbol{\theta}^{(i + 1)} \in \underset{\textbf{w}, \boldsymbol{\theta}}{\arg\max} \; \mathbb{E}_{\textbf{Z}|\textbf{X}}[\log p(\textbf{X}, \textbf{Z}; \textbf{w}, \boldsymbol{\theta})]
    \end{align}
    %
    \item[4.] Return to step 2, repeating this cycle until our likelihood converges. Note that the likelihood is guaranteed to (weakly) increase at each step using this procedure.
    \end{itemize}

It is also typical to re-start the procedure because we are guaranteed a local but not global optimum.

\subsection{The Math of EM}

The above sections supply an intuitive sense of why EM should work in scenarios where the latent variables follow categorical distributions. We now supply a more rigorous proof that iteratively computing the quantities derived above will increase the data log likelihood.

\subsubsection{The ELBO}

% Let the latent variables $\mathbf{z}$ follow a density $f_\theta$. As before, the observed data $\mathbf{x}$ then follow the the conditional distribution $p(\mathbf{x} \mid \mathbf{z}; \mathbf{w})$. 
Despite our inability to directly optimize the observed data log likelihood, it remains the quantity of interest. Note the following equivalence: 
\begin{align}
    \sum_{n = 1} ^N \log \left[\sum_{k = 1} ^K p(\mathbf{x}_n, \mathbf{z}_n = C_k; \boldsymbol\theta, \mathbf{w})\right] &= \sum_{n = 1} ^N \log \left[ \sum_{k = 1} ^K p(\mathbf{x}_n \mid \mathbf{z}_n = C_k; \boldsymbol\theta, \mathbf{w}) p(\mathbf{z}_n = C_k \mid \boldsymbol\theta, \mathbf{w} )\right] \\
    &= \sum_{n = 1} ^N \log \mathrm{E}_{\mathbf{z}_n \sim p(\mathbf{z}_n \mid \boldsymbol{\theta})} \left[ p(\mathbf{x}_n \mid \mathbf{z}_n, \mathbf{w})\right] \label{eq:expectation-log-likelihood}
\end{align}
We adopt this latter expression, since it also holds in cases where $\mathbf{z}_n$ is not discrete. 

As touched on earlier, the main issues with directly optimizing this expression are as follows:
\begin{enumerate}
    \item If we were to take a gradient, the distribution over which we're taking an expectation involves one of the parameters, $\boldsymbol\theta$, and hence computing the gradient with respect to $\boldsymbol{\theta}$ is difficult.
    \item There's an expectation (previously a sum) inside the logarithm.
\end{enumerate}

To solve this, we ultimately derive a lower bound on the observed data log likelihood which proves to be computationally tractable to optimize. First, let $q(\mathbf{z}_n)$ denote another probability distribution on $\mathbf{z}_n$. Then:
\begin{align}
    &\sum_{n = 1} ^N \log \left[\sum_{k = 1} ^K p(\mathbf{x}_n, \mathbf{z}_n = C_k; \boldsymbol\theta, \mathbf{w})\right] \\
    &= \sum_{n = 1} ^N \log \left[ \sum_{k = 1} ^K p(\mathbf{x}_n \mid \mathbf{z}_n = C_k; \boldsymbol\theta, \mathbf{w}) p(\mathbf{z}_n = C_k \mid \boldsymbol\theta, \mathbf{w} )\right] \\
    &= \sum_{n = 1} ^N \log \left[\sum_{k = 1} ^K p(\mathbf{x}_n \mid \mathbf{z}_n = C_k; \boldsymbol\theta, \mathbf{w}) p(\mathbf{z}_n = C_k \mid \boldsymbol\theta, \mathbf{w} ) \cdot \frac{q(\mathbf{z}_n = C_k)}{q(\mathbf{z}_n = C_k)}\right] \\
    &= \sum_{n = 1} ^N \log \left[\sum_{k = 1} ^K \frac{p(\mathbf{x}_n \mid \mathbf{z}_n = C_k; \boldsymbol\theta, \mathbf{w}) p(\mathbf{z}_n = C_k \mid \boldsymbol\theta, \mathbf{w} ) }{q(\mathbf{z}_n = C_k)}\cdot q(\mathbf{z}_n = C_k)\right] \\
    &= \sum_{n = 1} ^N \log \mathrm{E}_{\mathbf{z}_n \sim q(\mathbf{z}_n)} \left[\frac{p(\mathbf{x}_n \mid \mathbf{z}_n; \boldsymbol\theta, \mathbf{w}) p(\mathbf{z}_n \mid \boldsymbol\theta, \mathbf{w} ) }{q(\mathbf{z}_n)}\right]\label{eq:aux-expectation}
\end{align}

The above derivation again restricts to discrete $\mathbf{z}_n$, but the equivalence between the expressions in  \eqref{eq:expectation-log-likelihood} and \eqref{eq:aux-expectation} is in fact more general, holding whenever $p$ is absolutely continuous with respect to the chosen $q$. 

In any case, we've now fixed the first of the two issues. By introducing the distribution $q$, the expectation is no longer over some distribution depending on the parameter $\boldsymbol{\theta}$. 

To fix the second issue, we must somehow pass the log into the expectation. This is accomplished using Jensen's inequality, at the cost of turning the equality into a lower bound:
\begin{align}
    &\sum_{n = 1} ^N \log \mathrm{E}_{\mathbf{z}_n \sim p(\mathbf{z}_n \mid \boldsymbol{\theta})} \left[\frac{p(\mathbf{x}_n \mid \mathbf{z}_n; \boldsymbol\theta, \mathbf{w}) p(\mathbf{z}_n \mid \boldsymbol\theta, \mathbf{w} ) }{q(\mathbf{z}_n)}\right] \\
    &\qquad\geq \sum_{n = 1} ^N \mathrm{E}_{\mathbf{z}_n \sim p(\mathbf{z}_n \mid \boldsymbol{\theta})} \left[\log\left(\frac{p(\mathbf{x}_n \mid \mathbf{z}_n; \boldsymbol\theta, \mathbf{w}) p(\mathbf{z}_n \mid \boldsymbol\theta, \mathbf{w} ) }{q(\mathbf{z}_n)}\right)\right] 
\end{align}

In summary, the two issues with directly optimizing the observed data log likelihood were resolved as follows:
\begin{itemize}
    \item Introducing auxilliary variables $q(\mathbf{z}_n)$ allow us to reparametrize the expectation to be over $q$ rather than $p(\mathbf{z}_n \mid \boldsymbol{\theta})$. 
    \item Jensen's inequality allows the log to pass through the expectation.
\end{itemize}
However, this came at the cost of converting our objective into a lower bound of the original quantity, as well as introducing the new parameter that is the distribution $q$. 

It turns out that the iterative process given in the section above amounts to alternating between optimizing the parameters $\mathbf{w}, \mathbf{\theta}$ and then the distribution $q$. 

\subsubsection{Optimization}
Before delving into the optimization process, we first establish two identities involving the ELBO. 

First, 

\subsubsection{Correctness}

\subsubsection{Equivalence to Prior Formulation}

\subsection{Connection to K-Means Clustering}

At this point, it's worth considering the similarity between the EM algorithm and another coordinate ascent algorithm that we considered in the context of clustering: K-Means.

Recall that K-Means proceeds according to a similar iterative algorithm: we first make hard assignments of data points to existing cluster centers, and then we update the cluster centers based on the most recent data point assignments.

In fact, the main differences between K-Means clustering and the EM algorithm are that:
\begin{itemize}
    \item[1.] In the EM setting, we make soft cluster assignments through our $\textbf{q}_n$ values, rather than definitively assigning each data point to only one cluster.
    \item[2.] The EM algorithm is able to take advantage of flexible, class-conditional distributions 
      to capture the behavior of the observed data, whereas K-Means clustering relies only on distance measurements to make assignments and update cluster centers.
    \end{itemize}

    In the context of a mixture-of-Gaussian model, which we get to later in the chapter, we can confirm that K-means is equal to the limiting case of EM where the variance of each class-conditional Gaussian goes to 0, the prior probability of each class is uniform, and the distributions are spherical.

    \subsection{Dice Example: Mixture of Multinomials}
    
    Consider the following example scenario: we have two biased dice (with 6 faces) and one biased coin (with 2 sides). Data is generated as follows: first, the biased coin is flipped. Suppose it lands heads. Then dice 1 is rolled $c=10$ times. This gives the first example, i.e., $\mathbf{x}_1$ would correspond to the number of times of rolling each of a $1, 2, \ldots, 6$. Then we repeat, flipping the biased coin. Suppose it lands tails. Then Dice 2 is rolled 10 times. We record the result of the dice rolls as $\mathbf{x}_2$. We keep repeating, obtaining additional examples. 

   For example, our observations for the first 10 rolls may look like: 
    $    1, 5, 3, 4, 2, 2, 3, 1, 6, 2$ and we'd record as our first example
\begin{align*}
    \textbf{x}_1 =
        \begin{bmatrix}
            2 \\
            3 \\
            2 \\
            1 \\
            1 \\
            1
        \end{bmatrix} 
\end{align*}


We're going to try to infer the parameters of each of the dice based on these observations.
%
Let's consider how this scenario fits into our idea of a mixture model. First, the latent variable $\textbf{z}_n$ has a natural interpretation as being which dice was rolled for the $n^{th}$ observed data point $\textbf{x}_n$. We can represent $\textbf{z}_n$ using a one-hot vector, so that if the $n^{th}$ data point came from Dice 1, we'd denote that:
\begin{align*}
    \textbf{z}_n =
        \begin{bmatrix}
            1 \\
            0 \\
        \end{bmatrix} \\
\end{align*}

We denote the probability vector associated with the biased coin as $\boldsymbol{\theta} \in [0,1]^{2}$, summing to 1, with $\theta_1$ being the probability of the biased coin landing heads and $\theta_2$ being the probability of the biased coin landing tails. Furthermore, we need parameters to describe the behavior of the biased dice. We use $\boldsymbol{\pi_1}, \boldsymbol{\pi}_2 \in [0,1]^{6}$, summing to 1, where each 6-dimensional vector describes the probability that the respective dice lands on each face.

For a given dice, this defines a multinomial distribution. For $c$ trials, and counts $x_1, \ldots, x_6$ for each of 6 faces on a 6-sided dice, and probabilities $\boldsymbol{\pi}$, this is
%
\begin{align}
  p(\mathbf{x}; \boldsymbol{\pi})&=\frac{c!}{x_1!\cdot \ldots \cdot x_6!}\pi_1^{x_1}\cdot \ldots \cdot \pi_6^{x_6}
\end{align}

For our purposes, let $ p(\textbf{x}_n | \textbf{z}_n = C_k; \boldsymbol{\pi}_1, \boldsymbol{\pi}_2)$ denote the
multinomial distribution on observation $\textbf{x}_{nj}$ when latent vector $\textbf{z}_n = C_k$.

%
%
The model parameters are $\textbf{w} = \{\boldsymbol{\theta}, \boldsymbol{\pi}_1, \boldsymbol{\pi}_2 \}$.
We can optimize the model parameters using EM. We start by initializing the parameters $\boldsymbol{\theta}^{(0)}, \boldsymbol{\pi}^{(0)}$.

In the E-step, we compute the soft assignment values, $\textbf{q}_n$. For dice $k$, this given by
%
\begin{align} \label{E-for-multinomial}
q_{nk} = 
&  p(\textbf{z}_n = C_k | \textbf{x}_n; \boldsymbol{\theta}^{(i)}, \boldsymbol{\pi}^{(i)})
\\
&  =
     \frac{  p(\textbf{x}_n | \textbf{z}_n = C_k; \boldsymbol{\pi}^{(i)})p(\textbf{z}_n = C_k; \boldsymbol{\theta}^{(i)})}
     {\sum_{k=1}^K  p(\textbf{x}_n | \textbf{z}_n = C_k; \boldsymbol{\pi}^{(i)})p(\textbf{z}_n = C_k; \boldsymbol{\theta}^{(i)})}
  \\
  &  =
     \frac{  p(\textbf{x}_n | \textbf{z}_n = C_k; \boldsymbol{\pi}^{(i)})\theta_k^{(i)}}
     {\sum_{k=1}^K  p(\textbf{x}_n | \textbf{z}_n = C_k; \boldsymbol{\pi}^{(i)})\theta_k^{(i)}}
\end{align}


We could also use the ``product trick'' to write a single expression
%
\begin{align*}
  p(\textbf{z}_n  | \textbf{x}_n; \boldsymbol{\theta}^{(i)}, \boldsymbol{\pi}^{(i)})& =
 \frac{  \prod_{k=1}^K\left(  p(\textbf{x}_n | \textbf{z}_n = C_k; \boldsymbol{\pi}^{(i)})\theta_k^{(i)}\right) ^{z_{nk}}}
                                                                                      {\sum_{k=1}^K  p(\textbf{x}_n | \textbf{z}_n = C_k; \boldsymbol{\pi}^{(i)})\theta_k^{(i)}}
  \\
  & =
 \frac{  \prod_{k=1}^K p(\textbf{x}_n | \textbf{z}_n = C_k; \boldsymbol{\pi}^{(i)})^{z_{nk}}\prod_{k=1}^K(\theta_k^{(i)})^{z_{nk}}}
                                                                                      {\sum_{k=1}^K  p(\textbf{x}_n | \textbf{z}_n = C_k; \boldsymbol{\pi}^{(i)})\theta_k^{(i)}}
  \end{align*}

The vector $\textbf{q}_n$  is defined as:
%
\begin{align}
  \label{E-for-multinomial}
    \textbf{q}_n &= \begin{bmatrix}
                p(\textbf{z}_n = C_1 | \textbf{x}_n; \boldsymbol{\theta}^{(i)}, \boldsymbol{\pi}^{(i)}) \\
                p(\textbf{z}_n = C_2 | \textbf{x}_n; \boldsymbol{\theta}^{(i)}, \boldsymbol{\pi}^{(i)}) 
            \end{bmatrix} 
\end{align}


After computing the values of $\textbf{q}_n$, we are ready to perform the M-step.
Recall that we are maximizing the expected complete-data log likelihood, which takes the form:
%
\begin{align} \label{M-for-multinomial}
    \mathbb{E}_{\textbf{Z}|\textbf{X}}[\log p(\textbf{X}, \textbf{Z})] &= \mathbb{E}_{\textbf{q}_n} \bigg[\sum_{n=1}^{N} \log p(\textbf{z}_n; \boldsymbol{\theta}^{(i+1)}, \boldsymbol{\pi}^{(i+1)}) + \log p(\textbf{x}_n | \textbf{z}_n; \boldsymbol{\theta}^{(i+1)}, \boldsymbol{\pi}^{(i+1)})\bigg] \\ 
                                                                       &= \sum_{n=1}^{N} \mathbb{E}_{\textbf{z}_n|\textbf{x}_n} \bigg[ \log p(\textbf{z}_n; \boldsymbol{\theta}^{(i+1)}, \boldsymbol{\pi}^{(i+1)}) + \log p(\textbf{x}_n | \textbf{z}_n; \boldsymbol{\theta}^{(i+1)}, \boldsymbol{\pi}^{(i+1)})\bigg]
\end{align}

We can then substitute in for the multinomial expression and simplify,
and dropping constants we have that we're looking for
parameters that solve
%
\begin{align}
&  \arg\max_{\boldsymbol{\theta}^{(i+1)}, \boldsymbol{\pi}^{(i+1)}}\left\{
                \sum_{n=1}^{N} \sum_{k=1}^{2} q_{n, k}  \log \theta_k^{(i+1)} +
  \sum_{n=1}^{N} \sum_{k=1}^{2}q_{n,k}\log\left(\pi_{k,1}^{x_n,1}\cdot\ldots\cdot\pi_{k,6}^{x_n,6}\right) \right\}
 \notag \\
  &=  \arg\max_{\boldsymbol{\theta}^{(i+1)}, \boldsymbol{\pi}^{(i+1)}}\left\{
                \sum_{n=1}^{N} \sum_{k=1}^{2} q_{n, k}  \log \theta_k^{(i+1)} +
    \sum_{n=1}^{N} \sum_{k=1}^{2}\sum_{j=1}^6 q_{n,k}x_{n,j}\log(\pi_{k,j})\right\}
    \label{eq:dpnew1}
\end{align}

To maximize the expected complete-data log likelihood, it's necessary to introduce Lagrange multipliers to enforce the constraints $\sum_{k} \theta_k^{(i+1)} = 1$ and $\sum_{j} \pi_{k, j}^{(i+1)} = 1$, for each $k$. After doing this,
and solving, we recover the following update equations for the model parameters:
%
\begin{align*}
    \theta_{k}^{(i+1)} \leftarrow \frac{\sum_{n=1}^{N} q_{n, k}}{N}
\end{align*}
\begin{align*}
    \boldsymbol{\pi}_{k}^{(i+1)} \leftarrow \frac{\sum_{n=1}^{N} q_{n, k} \textbf{x}_{n}}{c \sum_{n=1}^{N} q_{n, k} },
\end{align*}
%
where $c=10$ in out example.

We now have everything we need to perform EM for this setup. After initializing our parameters $\textbf{w}^{(0)}$, we perform the E-step by evaluating \ref{E-for-multinomial}. After calculating our values of $\textbf{q}_n$ in the E-step, we update our parameters $\textbf{w} = \{\boldsymbol{\theta}, \boldsymbol{\pi}_1, \boldsymbol{\pi}_2 \}$ in the M-step by maximizing \ref{eq:dpnew1} with respect to $\boldsymbol{\theta}, \boldsymbol{\pi}_1, \boldsymbol{\pi}_2$. We perform these two steps iteratively, until convergence of our parameters. We may also do a restart.

\section{Gaussian Mixture Models (GMM)}
Our previous example was a simple but somewhat restricted application of the EM algorithm to solving a latent variable problem. We now turn to a more practical example, used widely in different contexts, called a Gaussian Mixture Model (GMM). As you might expect, a GMM consists of a combination of multiple Gaussian distributions. Among other things, it is useful for modeling scenarios where the observed data is continuous.

Let's go over a more rigorous formulation of the GMM setup. First, we have observed continous data $\textbf{x}_n \in \mathbb{R}^{m}$ and latent variables $\textbf{z}_n$ which indicate which Gaussian `cluster' our observed data point was drawn from. In other words:
\begin{align*}
    p(\textbf{x}_n | \textbf{z}_n = C_k) = \mathcal{N}(\textbf{x}_n; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\end{align*}
where $\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k$ are the mean and covariance parameters respectively for the $k^{th}$ cluster center.

The data generation process works as follows: we first sample a cluster center from a Categorical distribution parameterized by $\boldsymbol{\theta} \in \mathrm{R}^{K}$. Then, based on the sampled cluster center, we sample a data point $\textbf{x}_n \in \mathrm{R}^{m}$, which is the only piece of data that we actually observe. As usual for a mixture model, it is our goal to use the observed data to determine the cluster means and covariances, as well as the parameters of the Categorical distribution that selects the cluster centers.

Fortunately, this problem setup is perfectly suited for EM. We can apply the same machinery we've discussed throughout the chapter and used in the previous example.
\begin{itemize}
    \item[1.] First, we randomly initialize our parameters $\boldsymbol{\theta}, \{ \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \}_{k=1}^{K}$.
    \item[2.] [E-Step] Calculate the posterior distribution over $\textbf{z}_n$ given by $\textbf{q}_n$:
        \begin{align*}
            \textbf{q}_n = \mathrm{E}[\textbf{z}_n | \textbf{x}_n] &= \begin{bmatrix}
                p(\textbf{z}_n = C_1 | \textbf{x}_n; \theta_1, \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) \\
                \vdots \\
                p(\textbf{z}_n = C_K | \textbf{x}_n; \theta_K, \boldsymbol{\mu}_K, \boldsymbol{\Sigma}_K) \\
            \end{bmatrix} \\
            &\propto \begin{bmatrix}
                \theta_1 \mathcal{N}(\textbf{x}_n; \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) \\
                \vdots \\
                \theta_K \mathcal{N}(\textbf{x}_n; \boldsymbol{\mu}_K, \boldsymbol{\Sigma}_K) \\
            \end{bmatrix} \\
        \end{align*}
        This is the current expectation for our latent variables $\textbf{z}_n$ given our data $\textbf{x}_n$ and the current setting of our model parameters $\boldsymbol{\theta}, \{ \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \}_{k=1}^{K}$.
    \item[3.] [M-Step] Using our values of $\textbf{q}_n$, calculate the expected complete-data log likelihood, and then use that term to optimize our model parameters:
        \begin{align*}
            \mathbb{E}_{\textbf{q}_n}[\log p(\textbf{X}, \textbf{Z})] &= \mathbb{E}_{\textbf{q}_n} \bigg[ \sum_{n=1}^{N} \ln(p(\textbf{x}_n, \textbf{z}_n; \boldsymbol{\theta}, \{ \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \}_{k=1}^{K})) \bigg] \\
            &= \sum_{n=1}^{N} \sum_{k=1}^{K} q_{n, k} \ln \theta_k + q_{n, k} \ln \mathcal{N}(\textbf{x}_n; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \\
        \end{align*}
        We can then use this expected complete-data log likelihood to optimize our model parameters $\boldsymbol{\theta}, \{ \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k \}_{k=1}^{K}$ by computing the MLE as usual. Using a Lagrange multiplier to enforce $\sum_{k=1}^{K} \theta_k = 1$, we recover the update equations:
        \begin{align*}
            \theta_k^{(i + 1)} &\leftarrow \frac{\sum_{n=1}^{N} q_{n, k}}{N} \\
            \boldsymbol{\mu}_k^{(i + 1)} &\leftarrow \frac{\sum_{n=1}^{N} q_{n, k} \textbf{x}_n}{\sum_{n=1}^{N} q_{n, k}} \\
            \boldsymbol{\Sigma}_k^{(i + 1)} &\leftarrow \frac{\sum_{n=1}^{N} q_{n, k} (\textbf{x}_n - \boldsymbol{\mu}_k^{(i + 1)})(\textbf{x}_n - \boldsymbol{\mu}_k^{(i + 1)})^{T}}{\sum_{n=1}^{N} q_{n, k}} \\
        \end{align*}
    \item[4.] Return to step 2. Repeat until convergence.
\end{itemize}

Finally, it's worth comparing EM and K-Means clustering as applied to GMMs. First, as discussed previously, EM uses soft assignments of data points to clusters rather than hard assignments. Second, the standard K-Means algorithm does not estimate the covariance of each cluster. However, if we enforce as a part of our GMM setup that the covariance matrices of all the clusters are given by $\epsilon \textbf{I}$, then as $\epsilon \rightarrow 0$, EM and K-Means will in fact produce the same results.

\section{Admixture Models: Latent Dirichlet Allocation (LDA)}
With a grasp on mixture models, it is not too difficult to understand admixture models. In a sentence: an admixture model is a mixture of mixture models. Latent Dirichlet Allocation (LDA) is a common form of admixture models, and it is sometimes also referred to as \textit{topic modeling}, for reasons that will become apparent shortly. Describing LDA using an example will hopefully make the idea of an admixture model more concrete.

\subsection{LDA for Topic Modeling}
Consider the following data generating process for a set of text documents. We have a Dirichlet distribution $\boldsymbol{\theta} \sim Dir(\boldsymbol{\alpha})$ over the possible topics a document can take on.

\readernote{If you haven't seen the Dirichlet before, it is a distribution over an $n$-dimensional vector whose components sum to 1. For example, a sample from a dirichlet distribution in 3-dimensions could produce a sample that is the vector $$\begin{bmatrix} 0.2 \\ 0.5 \\ 0.3 \\ \end{bmatrix}$$.}

We sample from that Dirichlet distribution to determine the mixture of topics $\boldsymbol{\theta}_n$ in our document $D_n$:
\begin{align*}
    \boldsymbol{\theta}_n \sim Dir(\boldsymbol{\alpha})
\end{align*}
Then, for each possible topic, we sample from a Dirichlet distribution to determine the mixture of words $\boldsymbol{\phi}_k$ in that topic:
\begin{align*}
    \boldsymbol{\phi}_k \sim Dir(\boldsymbol{\beta})
\end{align*}
Then, for each word $\textbf{w}_{n, j}$ in the document $D_n$, we first sample from a Categorical parameterized by the topic mixture $\boldsymbol{\theta}_n$ to determine which topic that word will come from:
\begin{align*}
    \textbf{z}_{n, j} \sim Cat(\boldsymbol{\theta}_n)
\end{align*}
Then, now that we have a topic given by $\textbf{z}_{n, j}$ for this word $\textbf{w}_{n, j}$, we sample from a Categorical parameterized by that topic's mixture over words given by $\boldsymbol{\phi}_{\textbf{z}_{n, j}}$:
\begin{align*}
    \textbf{w}_{n, j} \sim Cat(\boldsymbol{\phi}_{\textbf{z}_{n, j}})
\end{align*}

Notice the mixture of mixtures at play here: we have a mixture model over the topics to produce each document in our corpus, and then for every word in a given document, we have a mixture over the topics to generate each individual word.

The indexing is particularly confusing because there are several layers of mixtures here, but to clarify: $n \in 1..N$ indexes each document $D_n$ in our corpus, $k \in 1..K$ indexes each possible topic, and $j \in 1..J$ indexes each word $\textbf{w}_{n,j}$ in document $D_n$, and $e \in 1..E$ indexes each word in our dictionary (note that $\textbf{w}_{n,j} \in \mathbb{R}^{E}$).

$\boldsymbol{\theta}_n$ specifies the distribution over topics in document $D_n$, and $\boldsymbol{\alpha}$ is the hyperparameter for the distribution that produces $\boldsymbol{\theta}_n$. Similarly, $\boldsymbol{\phi}_k$ specifies the distribution over words for the $k^{th}$ topic, and $\boldsymbol{\beta}$ is the hyperparameter for the distribution that produces $\boldsymbol{\phi}_k$.

\subsection{Applying EM to LDA}
Now that the problem setup and notation are taken care of, let's consider how we can apply EM to optimize the parameters $\boldsymbol{\theta}_n$ (the mixture over topics in a document) and $\boldsymbol{\phi}_k$ (the mixture over words for a topic). Note that we can simplify the problem slightly by considering $\boldsymbol{\theta}_n$ and $\boldsymbol{\phi}_k$ to be deterministic parameters for optimization (rather than random variables parameterized by $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$). Then, EM proceeds as follows:

\begin{itemize}
    \item[1.] First, we randomly initialize our parameters \{$\boldsymbol{\theta}_n \}_{n=1}^{N}, \{ \boldsymbol{\phi}_k\}_{k=1}^{K}$.
    \item[2.] [E-Step] Fix the topic distribution of the document given by $\boldsymbol{\theta}_n$ and the word distribution under a topic given by $\boldsymbol{\phi}_k$. Calculate the posterior distribution $\textbf{q}_{n, j} = p(\textbf{z}_{n, j} | \textbf{w}_{n, j})$, and note that this is the distribution over the possible topics of a word:
        \begin{align*}
            \textbf{q}_{n, j} = \mathrm{E}[\textbf{z}_{n, j} | \textbf{w}_{n, j}] &= \begin{bmatrix}
                p(\textbf{z}_{n, j} = C_1 | \textbf{w}_{n, j}; \boldsymbol{\theta}_n, \boldsymbol{\phi}_1) \\
                \vdots \\
                p(\textbf{z}_{n, j} = C_K | \textbf{w}_{n, j}; \boldsymbol{\theta}_n, \boldsymbol{\phi}_K) \\
            \end{bmatrix} \\
            &\propto \begin{bmatrix}
                p(\textbf{w}_{n, j} | \textbf{z}_{n, j} = C_1; \boldsymbol{\phi}_1) p(\textbf{z}_{n, j}  = C_1; \boldsymbol{\theta}_n) \\
                \vdots \\
                p(\textbf{w}_{n, j} | \textbf{z}_{n, j} = C_K; \boldsymbol{\phi}_K) p(\textbf{z}_{n, j}  = C_K; \boldsymbol{\theta}_n) \\
            \end{bmatrix} \\
            &= \begin{bmatrix}
                \phi_{1, \textbf{w}_{n, j}} \cdot \theta_{n, 1} \\
                \vdots \\
                \phi_{K, \textbf{w}_{n, j}} \cdot \theta_{n, K} \\
            \end{bmatrix} \\
        \end{align*}
    \item[3.] [M-Step] Using our values of $\textbf{q}_n$, calculate the expected complete-data log likelihood (which marginalizes over the unknown hidden variables $\textbf{z}_{n, j}$), and then use that expression to optimize our model parameters $\boldsymbol{\theta}_n$ and $\boldsymbol{\phi}_k$:
        \begin{align*}
            \mathbb{E}_{\textbf{q}_n}[\log p(\textbf{W}, \textbf{Z})] &= \mathbb{E}_{\textbf{q}_n} \bigg[ \sum_{n=1}^{N} \sum_{j=1}^{J} \ln(p(\textbf{w}_{n, j}, \textbf{z}_{n, j}; \{\boldsymbol{\theta}_n \}_{n=1}^{N}, \{ \boldsymbol{\phi}_k\}_{k=1}^{K} \bigg] \\
            &= \sum_{n=1}^{N} \sum_{j=1}^{J} \sum_{k=1}^{K} q_{n, j, k} \ln \theta_{n, k} + q_{n, j, k} \ln \phi_{k, \textbf{w}_{n, j}} \\
        \end{align*}
        We can then use this expected complete-data log likelihood to optimize our model parameters $\{\boldsymbol{\theta}_n \}_{n=1}^{N}, \{ \boldsymbol{\phi}_k\}_{k=1}^{K}$ by computing the MLE as usual. Using Lagrange multipliers to enforce $\forall n \sum_{k=1}^{K} \theta_{n, k} = 1$ and $\forall k \sum_{e=1}^{E} \phi_{k, e} = 1$ (where $e$ indexes each word in our dictionary), we recover the update equations:
        \begin{align*}
            \theta_{n, k}^{(i + 1)} &\leftarrow \frac{\sum_{j=1}^{J} q_{n, j, k}}{J} \\
            \phi_{k, d}^{(i + 1)} &\leftarrow \frac{\sum_{n=1}^{N} \sum_{j=1}^{J} q_{n, j, k} w_{n, j, d}}{\sum_{n=1}^{N} \sum_{j=1}^{J} q_{n, j, k}} \\
        \end{align*}
    \item[4.] Return to step 2. Repeat until convergence.
\end{itemize}

The largest headache for applying the EM algorithm to LDA is keeping all of the indices in order, and this is the result of working with a mixture of mixtures. Once the bookkeeping is sorted out, the actual updates are straightforward.

\section{Conclusion}
Mixture models are one common way of handling data that we believe is generated through a combination of unobserved, latent variables. We've seen that training these models directly is intractable (due to the marginalization over the latent variables), and so we turned to a coordinate ascent based algorithm known as Expectation-Maximization to get around this difficulty. We then explored a couple of common mixture models, including a multinomial mixture, Gaussian Mixture Model, and an admixture model known as Latent Dirichlet Allocation. Mixture models are a subset of a broader range of models known as latent variable models, and the examples seen in this chapter are just a taste of the many different mixture models available to us. Furthermore, EM is just a single algorithm for optimizing these models. A good grasp on the fundamentals of mixture models and the EM algorithm will be useful background for expanding to more complicated, expressive latent variable models.

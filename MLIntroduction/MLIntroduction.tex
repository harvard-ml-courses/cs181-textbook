\chapter{Introduction to Machine Learning}

\section{What is Machine Learning?}
There is a great deal of misunderstanding about what machine learning is, fueled by recent success and at times sensationalist media coverage. While its applications have been and will continue to be extraordinarily powerful under the right circumstances, it's important to gain some sense of where and why the tools presented in this book will be applicable. Broadly, machine learning is the application of statistical, mathematical, and numerical techniques to derive some form of knowledge from data. This `knowledge' may afford us some sort of summarization, visualization, grouping, or even predictive power over data sets.

With all that said, it's important to emphasize the limitations of machine learning. It is not nor will it ever be a replacement for critical thought and methodical, procedural work in data science. Indeed, machine learning can be reasonably characterized a loose collection of disciplines and tools. Where the lines begin that separate machine learning from statistics or mathematics or probability theory or any other handful of fields that it draws on are not clear. So while this book is a synopsis of the basics of machine learning, it might be better understood as a collection of tools that can be applied to a specific subset of problems.

\section{What Will This Book Teach Me?}
The purpose of this book is to provide you the reader with the following: a framework with which to approach problems that machine learning learning might help solve. You will hopefully come away with a sense of the strengths and weaknesses of the tools presented within and, even more importantly, gain an understanding of how these tools are situated among problems of interest. To that end, we will aim to develop systems for thinking about the structure of the problems we work on. That way, as we continue to add new tools and techniques to our repertoire, we will always have a clear view of the context in which we can expect to use them. This will not only create a nice categorization of the different practices in machine learning, it will also help motivate why these techniques exist in the first place.

You will not be an expert in any individual ML concept after reading this text. Rather, you should come away with three different levels of understanding. First, you should gain a general contextual awareness of the different problem types that ML techniques may be used to solve. Second, you should come away with a practical awareness of how different ML techniques operate. This means that after you have successfully identified an appropriate ML technique for a given problem, you will also know how that method actually accomplishes the goal at hand. If you only come away with these two levels of understanding, you will be off to a good start. The third level of understanding relates to having a derivational awareness of the algorithms and methods we will make use of. This level of understanding is not strictly necessary to successfully interact with existing machine learning capabilities, but it will be required if you desire to go further and deepen existing knowledge. Thus, we will be presenting derivations, but it will be secondary to a high level understanding of problem types and the practical intuition behind available solutions.

\section{Our Machine Learning Framework}

Let's consider for the first time what we will call the \textbf{Machine Learning Cube}. The purpose of this cube is to describe the domain of problems we will encounter, and it will be a useful way to delineate the techniques we will apply to different types of problems. Understanding the different facets of the cube will aid you in understanding machine learning as a whole, and can even give you intuition about techniques that you have never encountered before. Let's now describe the features of the cube.

Our cube has three axes. On the first axis we will put the output domain. The domain of our output can take on one of two forms: \textbf{discrete} or \textbf{continuous}. Discrete, or categorical data, is data that can only fall into one of $n$ specific classes. For example: male or female, integer values between 1 and 10, or different states in the U.S. are all examples of categorical data. Continuous data is that which falls on the real number line.

The second axis of the cube is reserved for the statistical nature of the machine learning technique in question. Specifically, it will fall into one of two broad categories: \textbf{probabilistic} or \textbf{non-probabilistic} techniques. Probabilistic techniques are those for which we incorporate our data using some form of statistical distribution or summary. In general, we are then able to discard some or all of our data once we have finished tuning our probabilistic model. In contrast, non-probabilistic techniques are those that use the data directly to perform some action. A very common and general example of this is comparing how close a new data point is to other points in your existing data set. Non-probabilistic techniques potentially make fewer assumptions, but they do require that you keep around some or all of your data. They are also potentially slower techniques at runtime because they may require touching all of the data in your dataset to perform some action. These are a very broad set of guidelines for the distinction between probabilistic and non-probabilistic techniques - you should expect to see some exceptions and even some techniques that fit into both of these categories to some degree. Having a sense for their general benefits and drawbacks is useful, and you will gain more intuition about the distinction as we begin to explore different techniques.

The third and final axis of the cube describes the type of training we will use. There are two major classes of machine learning techniques: \textbf{supervised} and \textbf{unsupervised}. In fact, these two classes of techniques are so important to describing the field of machine learning that we will roughly divide this textbook into two halves dedicated to techniques found within each of these categories. A supervised technique is one for which we get to observe a data set of both the inputs and the outputs ahead of time, to be used for training. For example, we might be given a data set about weather conditions and crop production over the years. Then, we could train a machine learning model that learns a relationship between the input data (weather) and output data (crop production). The implication here is that given new input data, we will be able to predict the unseen output data. An unsupervised technique is one for which we only get a data set of `inputs' ahead of time. In fact, we don't even need to consider these as inputs anymore: we can just consider them to be a set of data points that we wish to summarize or describe. Unsupervised techniques revolve around clustering or otherwise describing our data.

We will see examples of all of these as we progress throughout the book, and you will gain an intuition for where different types of data and techniques fall in our cube. Eventually, given just the information in the cube for a new technique, you will have a solid idea of how that technique operates.

\section{This Book's Notation}

The machine learning community uses a number of different conventions, and learning to decipher the different versions of those conventions is important to understanding work done in the field. For this book, we will try to stick to a standard notation that we define here in part. In addition to mathematical and statistical notation, we will also describe the conventions used in this book for explaining and breaking up different concepts.

\subsubsection{Mathematical and Statistical Notation}
We will describe the dimensionality of variables when necessary, but generic variables will often be sufficient when explaining new techniques. Boldface variables ($\textbf{x}$) represent vectors, capital boldface characters ($\textbf{X}$) represent matrices, and standard typeface variables ($x$) describe scalars.

Statistical distributions will sometimes be described in terms of their probability density function (PDF), e.g. $Y \sim \frac{1}{{\sigma \sqrt {2\pi } }}e^{{{ - \left( {x - \mu } \right)^2 } \mathord{\left/ {\vphantom {{ - \left( {y - \mu } \right)^2 } {2\sigma ^2 }}} \right. \kern-\nulldelimiterspace} {2\sigma ^2 }}}$. Alternatively, in the case of a well known probability distribution, we will describe those in terms of their standard notation, e.g. $Y \sim \mathcal{N}(\mu, \sigma^2)$

\subsubsection{Textbook Specific Notation}
We have also introduced a few conventions to make consumption of this material easier. We have boxes dedicated to definitions, explaining techniques in the context of the ML Framework Cube, and for explaining common misconceptions: \newline

\begin{definition}{Definition Explanation}{definition-explanation}
You will find definitions in these dark gray boxes.
\end{definition}

\begin{derivation}{Derivation Explanation}{derivation-explanation}
You will find derivations in these light gray boxes.
\end{derivation}

\begin{mlcube}{ML Framework Cube}
You will find ML Framework Cube explanations in these blue wrapped boxes.
\end{mlcube}

\readernote{You will find explanations for subtle or confusing concepts in these red wrapped boxes.}